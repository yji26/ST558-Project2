---
title: "ST 558 - Summer 2020 - Project 2"
author: "Yun Ji"
date: "7/3/2020"
output:
  rmarkdown::github_document:
    toc: true
params:
  day: ""
---

## Data Set Information
The news article popularity data set used in this project consists of 58 numerical predictor variables along with 2 informational fields; the target for the data set is the column `shares`, a measure of an article's popularity.

Seven columns from the data set are indicator values for the day of week that an article is published; these columns are named `weekday_is_monday`, `weekday_is_tuesday`, etc. For each weekday, I created a pair of linear and nonlinear regression models to predict the numerical value of `shares`. The code below loads the data set, filters on a specific day of week according to the parameter variable `params$day`, then splits the resulting data 70/30 into training and testing data sets. The training set is used for training the two regression models, and the testing set is used for comparing which of the models has a higher prediction accuracy.

First I load the raw data from a stored file in my repository into a data frame in R.

```{r load news data}
newsData <- read_csv("./Data/OnlineNewsPopularity.csv")
```

Some columns of the data frame may be removed to reduce the size of the data. Column `url` merely contains the text URL for the article and is not needed. Column `timedelta` represents the number of days between article publication and data acquisition; although this can potentially be used to detect the site's popularity over time, the data dictionary describes this as non-predictive data and therefore I also exclude it from the modeling data. Column `is_weekend` is made redundant when the data is split by day of week, since for any given day its column value will be either all zeroes or all ones, and therefore not useful for prediction. Columns `rate_positive_words` and `rate_negative_words` represent the proportion of positive and negative words among all non-neutral words, and always sum to 1; one of them may be removed without any loss of information, and I choose to remove `rate_negative_words`.

```{r remove columns}
newsData <- newsData %>%
  select(!url & !timedelta & !is_weekend & !rate_negative_words)
```

## Filter for `r params$day` Data
Next I filter rows for the day of week based on parameter value `params$day`, then remove the `weekday_is_*` columns from the data frame. This is the resulting data set right before being split into training and testing sets.

```{r day of week filtering}
dayOfWeek <- params$day
dayColumn <- paste0("weekday_is_", tolower(dayOfWeek))

newsDataFiltered <- newsData %>%
  filter((!!as.symbol(dayColumn)) == 1) %>%
  select(!starts_with("weekday_is_"))
```

## Exploratory Data Analysis
Inputs to the models require that all columns have numerical non-null values. Make sure this is true.

```{r data validation}
summary(newsDataFiltered)
```

Indeed, it can be verified that the data fit these criteria, so no further data cleaning is required. The data is not the same from one day of week to another, but certain observations hold across all.

First, let us examine the distribution of the target variable `shares`.

```{r shares histogram}
g <- ggplot(data = newsDataFiltered, aes(x = shares))
g + geom_histogram(bins = 50) +
  labs(title = paste0("Histogram of Shares for ", dayOfWeek),
       x = "Shares", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))
```

Values of `shares` appear to follow a power-law distribution, having a long tail consisting of a few very popular articles that receive an outsized share of views. Therefore for regression it may be better to transform the target variable using a logarithmic function.

```{r target transformation}
newsDataFiltered <- newsDataFiltered %>%
  mutate(shares = log10(shares))

g <- ggplot(data = newsDataFiltered, aes(x = shares))
g + geom_histogram(bins = 50) +
  labs(title = paste0("Histogram of Log-Adjusted Shares for ", dayOfWeek),
       x = "Log10 of Shares", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))
```

The log-adjusted distribution shows that news articles with middling scores are most frequent, and the frequency tapers off as the shares go to the extremes on either side. With this distribution for the target variable, a regression model (rather than classification model with two categories) would be appropriate.

From the summary of the data frame, we observe that the range of values for the predictor columns vary: some columns such as `global_subjectivity` are proportions and are limited to continuous values between 0 and 1, some indicator values like `data_channel_is_world` have only integer values 0 or 1, and others like `n_tokens_content` are raw counts which are natural numbers with no theoretical upper bound.

```{r predictor histograms}
g <- ggplot(data = newsDataFiltered, aes(x = global_subjectivity))
g + geom_histogram(bins = 10) +
  labs(title = paste0("Histogram of global_subjectivity for ", dayOfWeek),
       y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

g <- ggplot(data = newsDataFiltered, aes(x = data_channel_is_world))
g + geom_histogram(bins = 2) +
  labs(title = paste0("Histogram of data_channel_is_world for ", dayOfWeek),
       y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))

g <- ggplot(data = newsDataFiltered, aes(x = n_tokens_content))
g + geom_histogram(bins = 50) +
  labs(title = paste0("Histogram of n_tokens_content for ", dayOfWeek),
       y = "Count") +
  theme(plot.title = element_text(hjust = 0.5))
```

Because of this, when selecting for models, it is advised to standardize (that is, center and scale) all predictor values prior to fitting the models. With this many predictor variables it would be difficult to tease out strong univariate relationships between a single predictor and the target variable, regardless of other predictor values. Therefore it is advisible to not exclude any predictor variable from the models. And while there is a chance of overfitting, especially for the nonlinear model, if tuning parameters and methodology are well-chosen, this risk may be minimized.

As a last step before modeling, I split the news data into training and testing sets in a 70-to-30 proportion.

```{r train test split}
set.seed(seed)
train <- sample(1:nrow(newsDataFiltered), size = nrow(newsDataFiltered)*0.7)
test <- dplyr::setdiff(1:nrow(newsDataFiltered), train)
newsDataTrain <- newsDataFiltered[train, ]
newsDataTest <- newsDataFiltered[test, ]
```

## Modeling and Cross-Validation
Two regression models are fitted to the training data: a multiple linear regression model and a nonlinear Random Forest regression model. To tune these models, I perform three repeated 5-fold cross-validations from the `caret` package. Cross-validation is used to limit overfitting on the training data, because in each CV one of the folds is held out as a validation set while the remaining folds are combined and used to train. Besides centering and scaling, the linear regression model uses the package default options, while for the Random Forest model, I specified tree depths to be 3, 5 or 10, because deeper trees can be prone to overfitting and take up a lot of computation time.

```{r model fitting}
set.seed(seed)
trctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

lm_fit <- train(shares ~ ., 
                data = newsDataTrain, 
                method = "lm",
                trControl = trctrl,
                preProcess = c("center", "scale")
)

rf_fit <- train(shares ~ ., 
                data = newsDataTrain, 
                method = "rf",
                trControl = trctrl,
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(mtry = c(3, 5, 10))
)
```

Comparing the performance of the models on the training set, we get the following:
```{r model training performance}
lm_fit$results

rf_fit$results
```

The model with higher `Rsquared` value and lower `RMSE` value is the better performer on the training data (which model is better may vary depending on the day of week used). However, the real test comes when the fitted models are evaluated for prediction accuracy on the testing data set.

## Model Test Performance

```{r model testing performance}
newsLmPred <- predict(lm_fit, newdata = newsDataTest)
lm_rmspe <- sqrt(mean((newsDataTest$shares - newsLmPred)^2))
lm_rmspe

newsRfPred <- predict(rf_fit, newdata = newsDataTest)
rf_rmspe <- sqrt(mean((newsDataTest$shares - newsRfPred)^2))
rf_rmspe
```

Here we compare the root mean-square prediction error on the log-scaled target variable `shares`: the multiple linear regression model has RMSPE of `r round(lm_rmspe, 5)` and the Random Forest regression model has RMSPE of `r round(rf_rmspe, 5)`. The model with lower RMSPE is the better performer on the testing data set.

To test whether the models could be overfit for the training data, we compare the root mean-square prediction error against the root mean-square error from the training data set. For the linear regression model the training RMSE is `r round(lm_fit$results$RMSE, 5)` and for the Random Forest model the RMSE is `r round(min(rf_fit$results$RMSE), 5)`. For the models to not be overfit, each model's training RMSE and testing RMSPE values ought to be close to each other.
